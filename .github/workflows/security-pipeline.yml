name: Optimized Flutter Security Pipeline

on:
  push:
    branches: [main, develop]
    paths:
      - 'lib/**'
      - 'test/**'
      - 'pubspec.yaml'
      - 'pubspec.lock'
      - '.github/workflows/**'
      - 'scripts/**'
      
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]

  schedule:
    - cron: '0 0 * * 1'  # Weekly security scan
    
  workflow_dispatch:
    inputs:
      scan_type:
        description: 'Scan type'
        required: true
        default: 'full'
        type: choice
        options:
          - incremental
          - full
          - critical-only

concurrency:
  group: security-${{ github.workflow }}-${{ github.ref_name }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

permissions:
  contents: read
  id-token: write
  security-events: write
  actions: read

env:
  # Core versions
  FLUTTER_VERSION: '3.19.0'
  DART_VERSION: '3.3.0'
  
  # Security tool versions
  SYFT_VERSION: 'v1.29.0'
  GRYPE_VERSION: 'v0.74.1'
  TRIVY_VERSION: 'v0.48.1'
  SEMGREP_VERSION: 'v1.45.0'
  
  # Security configuration
  GRYPE_FAIL_ON_SEVERITY: high
  TRIVY_SEVERITY: CRITICAL,HIGH,MEDIUM
  
  # Timeout settings
  TIMEOUT_MINUTES: 30

jobs:
  # Pre-check - Fast fail
  pre-check:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
      scan_type: ${{ steps.check.outputs.scan_type }}
      changed_files: ${{ steps.check.outputs.changed_files }}
      base_sha: ${{ steps.check.outputs.base_sha }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - id: check
        run: |
          # Determine scan type and check if should run
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            BASE_SHA="${{ github.event.pull_request.base.sha }}"
            CURRENT_SHA="${{ github.sha }}"
          else
            BASE_SHA="${{ github.event.before }}"
            CURRENT_SHA="${{ github.sha }}"
          fi
          echo "base_sha=$BASE_SHA" >> $GITHUB_OUTPUT
          # Override with manual input if provided
          if [[ -n "${{ github.event.inputs.scan_type }}" ]]; then
            echo "scan_type=${{ github.event.inputs.scan_type }}" >> $GITHUB_OUTPUT
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "scan_type=full" >> $GITHUB_OUTPUT
            echo "should_run=true" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            if [[ "$BASE_SHA" != "0000000000000000000000000000000000000000" ]]; then
              CHANGED=$(git diff --name-only $BASE_SHA $CURRENT_SHA | grep -E '\.(dart|yaml)$' | wc -l)
              if [ $CHANGED -eq 0 ]; then
                echo "should_run=false" >> $GITHUB_OUTPUT
              else
                echo "should_run=true" >> $GITHUB_OUTPUT
                echo "scan_type=incremental" >> $GITHUB_OUTPUT
                CHANGED_FILES=$(git diff --name-only $BASE_SHA $CURRENT_SHA | grep -E '\.(dart|yaml)$' || true)
                echo "changed_files<<EOF" >> $GITHUB_OUTPUT
                echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
                echo "EOF" >> $GITHUB_OUTPUT
              fi
            else
              echo "should_run=true" >> $GITHUB_OUTPUT
              echo "scan_type=full" >> $GITHUB_OUTPUT
            fi
          else
            echo "should_run=true" >> $GITHUB_OUTPUT
            echo "scan_type=full" >> $GITHUB_OUTPUT
          fi

  # Cache setup
  setup-and-cache:
    runs-on: ubuntu-latest
    needs: pre-check
    if: needs.pre-check.outputs.should_run == 'true'
    outputs:
      cache-hit: ${{ steps.cache.outputs.cache-hit }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Install parsing helpers
        run: |
          sudo apt-get update && sudo apt-get install -y jq python3-yaml
          wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/local/bin/yq
          chmod +x /usr/local/bin/yq
          # Retry function for network operations
          retry() {
            local n=1
            local max=3
            local delay=5
            while true; do
              "$@" && break || {
                if [[ $n -lt $max ]]; then
                  ((n++))
                  echo "Command failed. Attempt $n/$max:"
                  sleep $delay;
                else
                  echo "The command has failed after $n attempts."
                  return 1
                fi
              }
            done
          }
          
      - name: Update security databases
        run: |
          # Install and update Grype database
          wget -qO- https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh
          export PATH="$PWD/bin:$PATH"
          grype db update
          
      - name: Enhanced Cache Strategy
        id: cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.pub-cache
            .dart_tool
            .packages
            ~/.cache/semgrep
            ~/.cache/trivy
            ~/.grype/db
            ~/.syft/db
            ~/security-tools
          key: ${{ runner.os }}-flutter-${{ env.FLUTTER_VERSION }}-${{ hashFiles('**/pubspec.lock') }}
          restore-keys: |
            ${{ runner.os }}-flutter-${{ env.FLUTTER_VERSION }}-
            ${{ runner.os }}-flutter-

  # Independent SBOM generation
  generate-sbom:
    runs-on: ubuntu-latest
    needs: [pre-check, setup-and-cache]
    if: needs.pre-check.outputs.should_run == 'true'
    timeout-minutes: 10
    
    steps:
      - uses: actions/checkout@v4
        
      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          
      - name: Get dependencies
        run: flutter pub get
        
      - name: Generate comprehensive SBOM
        run: |
          # Install Syft
          curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
          syft --version
          
          # Generate Syft SBOM
          syft . --output cyclonedx-json=syft-sbom.json
          
          # Generate enhanced Flutter-specific SBOM
          cat > generate-enhanced-flutter-sbom.dart << 'EOF'
          import 'dart:io';
          import 'dart:convert';
          import 'dart:convert' show utf8;
          
          void main() async {
            try {
              // Run flutter pub deps command with detailed output
              final result = await Process.run('flutter', ['pub', 'deps', '--json']);
              if (result.exitCode != 0) {
                print('Error running flutter pub deps: ${result.stderr}');
                exit(1);
              }
              
              final depsData = json.decode(result.stdout);
              
              // Read pubspec.yaml for metadata
              final pubspecFile = File('pubspec.yaml');
              final pubspecContent = await pubspecFile.readAsString();
              
              // Enhanced parsing of pubspec.yaml
              Map<String, dynamic> pubspecInfo = parsePubspecYaml(pubspecContent);
              
              // Generate enhanced SBOM
              final sbom = {
                'bomFormat': 'CycloneDX',
                'specVersion': '1.4',
                'serialNumber': 'urn:uuid:${DateTime.now().millisecondsSinceEpoch}',
                'version': 1,
                'metadata': {
                  'timestamp': DateTime.now().toIso8601String(),
                  'tools': [{
                    'vendor': 'Flutter Security Scanner',
                    'name': 'enhanced-flutter-sbom-generator',
                    'version': '2.0.0'
                  }],
                  'component': {
                    'type': 'application',
                    'bom-ref': pubspecInfo['name'] ?? 'unknown',
                    'name': pubspecInfo['name'] ?? 'unknown',
                    'version': pubspecInfo['version'] ?? '0.0.0',
                    'description': pubspecInfo['description'] ?? '',
                    'licenses': pubspecInfo['license'] != null ? [{
                      'license': {
                        'id': pubspecInfo['license'],
                        'name': pubspecInfo['license']
                      }
                    }] : [],
                    'properties': [
                      {
                        'name': 'flutter:version',
                        'value': await getFlutterVersion()
                      },
                      {
                        'name': 'dart:version',
                        'value': await getDartVersion()
                      },
                      {
                        'name': 'build:platforms',
                        'value': pubspecInfo['platforms']?.toString() ?? '{}'
                      }
                    ]
                  }
                },
                'components': []
              };
              
              // Process dependencies with enhanced metadata
              final components = <Map<String, dynamic>>[];
              final processedPackages = <String>{};
              
              for (var package in depsData['packages']) {
                final packageName = package['name'];
                final packageVersion = package['version'];
                final packageKey = '$packageName@$packageVersion';
                
                // Avoid duplicates
                if (processedPackages.contains(packageKey)) continue;
                processedPackages.add(packageKey);
                
                if (packageName != pubspecInfo['name']) {
                  final component = {
                    'type': 'library',
                    'bom-ref': 'pkg:pub/$packageName@$packageVersion',
                    'name': packageName,
                    'version': packageVersion,
                    'purl': 'pkg:pub/$packageName@$packageVersion',
                    'scope': package['kind'] == 'direct' ? 'required' : 'optional',
                    'properties': [
                      {
                        'name': 'dependency:type',
                        'value': package['kind'] ?? 'unknown'
                      },
                      {
                        'name': 'dependency:source',
                        'value': package['source'] ?? 'hosted'
                      }
                    ]
                  };
                  
                  // Add license information if available
                  if (package['license'] != null) {
                    component['licenses'] = [{
                      'license': {
                        'id': package['license'],
                        'name': package['license']
                      }
                    }];
                  }
                  
                  // Add author information if available
                  if (package['authors'] != null) {
                    component['authors'] = package['authors'];
                  }
                  
                  // Add description if available
                  if (package['description'] != null) {
                    component['description'] = package['description'];
                  }
                  
                  // Enhance Git dependencies with commit hash
                  await enhanceGitDependencies(component);
                  
                  // Add package hashes for integrity
                  await addPackageHashes(component);
                  
                  components.add(component);
                }
              }
              
              // Add transitive dependencies
              await addTransitiveDependencies(components, processedPackages);
              
              // Add native dependencies
              await addNativeDependencies(components, processedPackages);
              
              // Add build dependencies
              await addBuildDependencies(components, processedPackages, pubspecInfo);
              
              // Add Web platform dependencies
              await addWebDependencies(components, processedPackages);
              
              // Add platform-specific dependencies
              await addPlatformSpecificDependencies(components, processedPackages);
              
              // Add plugin native dependencies
              await addPluginNativeDependencies(components, processedPackages);
              
              // Add runtime dependencies
              await addRuntimeDependencies(components, processedPackages);
              
              // Add build toolchain
              await addBuildToolchain(sbom);
              
              sbom['components'] = components;
              
              // Write enhanced SBOM
              final sbomFile = File('enhanced-flutter-sbom.json');
              await sbomFile.writeAsString(
                JsonEncoder.withIndent('  ').convert(sbom)
              );
              print('Enhanced Flutter SBOM generated successfully');
              print('Total components: ${components.length}');
              print('Processed packages: ${processedPackages.length}');
            } catch (e, stack) {
              print('Error generating enhanced Flutter SBOM: $e');
              print('Stack trace: $stack');
              exit(1);
            }
          }
          
          // Enhanced YAML parsing
          Map<String, dynamic> parsePubspecYaml(String content) {
            final lines = content.split('\n');
            final info = <String, dynamic>{};
            String currentSection = '';
            
            for (final line in lines) {
              final trimmedLine = line.trim();
              if (trimmedLine.isEmpty) continue;
              
              if (trimmedLine.startsWith('name:')) {
                info['name'] = trimmedLine.split(':')[1].trim();
              } else if (trimmedLine.startsWith('version:')) {
                info['version'] = trimmedLine.split(':')[1].trim().split('+')[0];
              } else if (trimmedLine.startsWith('description:')) {
                info['description'] = trimmedLine.split(':')[1].trim();
              } else if (trimmedLine.startsWith('license:')) {
                info['license'] = trimmedLine.split(':')[1].trim();
              } else if (trimmedLine.startsWith('authors:')) {
                info['authors'] = trimmedLine.split(':')[1].trim();
              } else if (trimmedLine.startsWith('platforms:')) {
                info['platforms'] = {};
                currentSection = 'platforms';
              } else if (currentSection == 'platforms' && trimmedLine.startsWith('  ')) {
                final platformLine = trimmedLine.trim();
                if (platformLine.contains(':')) {
                  final parts = platformLine.split(':');
                  info['platforms'][parts[0].trim()] = parts[1].trim();
                }
              } else if (trimmedLine.startsWith('dependencies:') || 
                         trimmedLine.startsWith('dev_dependencies:')) {
                currentSection = trimmedLine.startsWith('dependencies:') ? 'dependencies' : 'dev_dependencies';
                info[currentSection] = {};
              } else if ((currentSection == 'dependencies' || currentSection == 'dev_dependencies') && 
                         trimmedLine.startsWith('  ')) {
                final depLine = trimmedLine.trim();
                if (depLine.contains(':')) {
                  final parts = depLine.split(':');
                  final depName = parts[0].trim();
                  final depVersion = parts[1].trim();
                  info[currentSection][depName] = depVersion;
                }
              }
            }
            
            return info;
          }
          
          // Get Flutter version
          Future<String> getFlutterVersion() async {
            try {
              final result = await Process.run('flutter', ['--version']);
              if (result.exitCode == 0) {
                final lines = result.stdout.toString().split('\n');
                for (final line in lines) {
                  if (line.contains('Flutter')) {
                    final match = RegExp(r'Flutter (\d+\.\d+\.\d+)').firstMatch(line);
                    if (match != null) {
                      return match.group(1)!;
                    }
                  }
                }
              }
            } catch (e) {
              print('Error getting Flutter version: $e');
            }
            return 'unknown';
          }
          
          // Get Dart version
          Future<String> getDartVersion() async {
            try {
              final result = await Process.run('dart', ['--version']);
              if (result.exitCode == 0) {
                final lines = result.stdout.toString().split('\n');
                for (final line in lines) {
                  if (line.contains('Dart VM version')) {
                    final match = RegExp(r'Dart VM version: (\d+\.\d+\.\d+)').firstMatch(line);
                    if (match != null) {
                      return match.group(1)!;
                    }
                  }
                }
              }
            } catch (e) {
              print('Error getting Dart version: $e');
            }
            return 'unknown';
          }
          
          // Add transitive dependencies
          Future<void> addTransitiveDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final packageConfigFile = File('.dart_tool/package_config.json');
            if (await packageConfigFile.exists()) {
              try {
                final packageConfig = json.decode(await packageConfigFile.readAsString());
                for (var package in packageConfig['packages']) {
                  final packageName = package['name'];
                  final packagePath = package['rootUri'];
                  
                  if (packagePath != null && packagePath.startsWith('file://')) {
                    final pubspecPath = packagePath.replaceFirst('file://', '') + '/pubspec.yaml';
                    final pubspecFile = File(pubspecPath);
                    
                    if (await pubspecFile.exists()) {
                      final pubspecContent = await pubspecFile.readAsString();
                      final pubspecInfo = parsePubspecYaml(pubspecContent);
                      
                      // Add transitive dependencies
                      if (pubspecInfo['dependencies'] != null) {
                        for (final entry in pubspecInfo['dependencies'].entries) {
                          final depName = entry.key;
                          final depVersion = entry.value;
                          final packageKey = '$depName@$depVersion';
                          
                          if (!processedPackages.contains(packageKey)) {
                            processedPackages.add(packageKey);
                            components.add({
                              'type': 'library',
                              'bom-ref': 'pkg:pub/$depName@$depVersion',
                              'name': depName,
                              'version': depVersion,
                              'purl': 'pkg:pub/$depName@$depVersion',
                              'scope': 'optional',
                              'properties': [
                                {
                                  'name': 'dependency:type',
                                  'value': 'transitive'
                                },
                                {
                                  'name': 'source:package',
                                  'value': packageName
                                }
                              ]
                            });
                          }
                        }
                      }
                    }
                  }
                }
              } catch (e) {
                print('Error processing transitive dependencies: $e');
              }
            }
          }
          
          // Add native dependencies
          Future<void> addNativeDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            // Check for Android native dependencies
            final androidDir = Directory('android');
            if (await androidDir.exists()) {
              final gradleFile = File('android/app/build.gradle');
              if (await gradleFile.exists()) {
                final content = await gradleFile.readAsString();
                final dependencies = RegExp(r"implementation\s+['\"]([^'\"]+)['\"]").allMatches(content);
                
                for (final match in dependencies) {
                  final dep = match.group(1)!;
                  final parts = dep.split(':');
                  if (parts.length >= 2) {
                    final group = parts[0];
                    final name = parts[1];
                    final version = parts.length > 2 ? parts[2] : 'unknown';
                    final packageKey = '$group:$name@$version';
                    
                    if (!processedPackages.contains(packageKey)) {
                      processedPackages.add(packageKey);
                      components.add({
                        'type': 'library',
                        'bom-ref': 'pkg:maven/$group/$name@$version',
                        'name': name,
                        'version': version,
                        'purl': 'pkg:maven/$group/$name@$version',
                        'scope': 'required',
                        'properties': [
                          {
                            'name': 'platform',
                            'value': 'android'
                          },
                          {
                            'name': 'group',
                            'value': group
                          }
                        ]
                      });
                    }
                  }
                }
              }
            }
            
            // Check for iOS native dependencies
            final iosDir = Directory('ios');
            if (await iosDir.exists()) {
              final podfile = File('ios/Podfile');
              if (await podfile.exists()) {
                final content = await podfile.readAsString();
                final dependencies = RegExp(r"pod\s+['\"]([^'\"]+)['\"](?:\s*,\s*['\"]([^'\"]+)['\"])?").allMatches(content);
                
                for (final match in dependencies) {
                  final name = match.group(1)!;
                  final version = match.group(2) ?? 'unknown';
                  final packageKey = '$name@$version';
                  
                  if (!processedPackages.contains(packageKey)) {
                    processedPackages.add(packageKey);
                    components.add({
                      'type': 'library',
                      'bom-ref': 'pkg:cocoapods/$name@$version',
                      'name': name,
                      'version': version,
                      'purl': 'pkg:cocoapods/$name@$version',
                      'scope': 'required',
                      'properties': [
                        {
                          'name': 'platform',
                          'value': 'ios'
                        }
                      ]
                    });
                  }
                }
              }
            }
          }
          
          // Add build dependencies
          Future<void> addBuildDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages, Map<String, dynamic> pubspecInfo) async {
            if (pubspecInfo['dev_dependencies'] != null) {
              for (final entry in pubspecInfo['dev_dependencies'].entries) {
                final name = entry.key;
                final version = entry.value;
                final packageKey = '$name@$version';
                
                if (!processedPackages.contains(packageKey)) {
                  processedPackages.add(packageKey);
                  components.add({
                    'type': 'library',
                    'bom-ref': 'pkg:pub/$name@$version',
                    'name': name,
                    'version': version,
                    'purl': 'pkg:pub/$name@$version',
                    'scope': 'optional',
                    'properties': [
                      {
                        'name': 'dependency:type',
                        'value': 'dev_dependency'
                      },
                      {
                        'name': 'build:tool',
                        'value': 'true'
                      }
                    ]
                  });
                }
              }
            }
          }
          
          // Add Web platform dependencies
          Future<void> addWebDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final webDir = Directory('web');
            if (await webDir.exists()) {
              final indexFile = File('web/index.html');
              if (await indexFile.exists()) {
                final content = await indexFile.readAsString();
                
                // Detect CDN referenced JS libraries
                final scriptTags = RegExp(r'<script[^>]+src="([^"]+)"[^>]*>').allMatches(content);
                for (final match in scriptTags) {
                  final src = match.group(1)!;
                  if (src.contains('cdn') || src.startsWith('http')) {
                    final libraryName = extractLibraryName(src);
                    final version = extractVersion(src) ?? 'unknown';
                    final packageKey = '$libraryName@$version';
                    
                    if (!processedPackages.contains(packageKey)) {
                      processedPackages.add(packageKey);
                      components.add({
                        'type': 'library',
                        'bom-ref': 'pkg:cdn/$libraryName@$version',
                        'name': libraryName,
                        'version': version,
                        'purl': 'pkg:generic/$libraryName@$version',
                        'scope': 'required',
                        'properties': [
                          {
                            'name': 'platform',
                            'value': 'web'
                          },
                          {
                            'name': 'source',
                            'value': 'cdn'
                          },
                          {
                            'name': 'url',
                            'value': src
                          }
                        ]
                      });
                    }
                  }
                }
              }
            }
          }
          
          // Extract library name from CDN URL
          String extractLibraryName(String url) {
            final uri = Uri.parse(url);
            final path = uri.path;
            final fileName = path.split('/').last;
            return fileName.split('.').first;
          }
          
          // Extract version from CDN URL
          String? extractVersion(String url) {
            final versionMatch = RegExp(r'/(\d+\.\d+\.\d+)/').firstMatch(url);
            return versionMatch?.group(1);
          }
          
          // Add platform-specific dependencies
          Future<void> addPlatformSpecificDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            // Linux platform
            final linuxDir = Directory('linux');
            if (await linuxDir.exists()) {
              await parseLinuxDependencies(components, processedPackages);
            }
            
            // Windows platform
            final windowsDir = Directory('windows');
            if (await windowsDir.exists()) {
              await parseWindowsDependencies(components, processedPackages);
            }
            
            // macOS platform
            final macosDir = Directory('macos');
            if (await macosDir.exists()) {
              await parseMacOSDependencies(components, processedPackages);
            }
          }
          
          // Parse Linux dependencies
          Future<void> parseLinuxDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final cmakeFile = File('linux/CMakeLists.txt');
            if (await cmakeFile.exists()) {
              final content = await cmakeFile.readAsString();
              final dependencies = RegExp(r'find_package\(([^)]+)\)').allMatches(content);
              
              for (final match in dependencies) {
                final depName = match.group(1)!;
                final packageKey = '$depName@unknown';
                
                if (!processedPackages.contains(packageKey)) {
                  processedPackages.add(packageKey);
                  components.add({
                    'type': 'library',
                    'bom-ref': 'pkg:cmake/$depName',
                    'name': depName,
                    'version': 'unknown',
                    'purl': 'pkg:cmake/$depName',
                    'scope': 'required',
                    'properties': [
                      {
                        'name': 'platform',
                        'value': 'linux'
                      }
                    ]
                  });
                }
              }
            }
          }
          
          // Parse Windows dependencies
          Future<void> parseWindowsDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final cmakeFile = File('windows/flutter/generated_plugins.cmake');
            if (await cmakeFile.exists()) {
              final content = await cmakeFile.readAsString();
              final dependencies = RegExp(r'list\(APPEND FLUTTER_PLUGIN_LIST ([^)]+)\)').allMatches(content);
              
              for (final match in dependencies) {
                final depName = match.group(1)!;
                final packageKey = '$depName@unknown';
                
                if (!processedPackages.contains(packageKey)) {
                  processedPackages.add(packageKey);
                  components.add({
                    'type': 'library',
                    'bom-ref': 'pkg:cmake/$depName',
                    'name': depName,
                    'version': 'unknown',
                    'purl': 'pkg:cmake/$depName',
                    'scope': 'required',
                    'properties': [
                      {
                        'name': 'platform',
                        'value': 'windows'
                      }
                    ]
                  });
                }
              }
            }
          }
          
          // Parse macOS dependencies
          Future<void> parseMacOSDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final podfile = File('macos/Podfile');
            if (await podfile.exists()) {
              final content = await podfile.readAsString();
              final dependencies = RegExp(r"pod\s+['\"]([^'\"]+)['\"](?:\s*,\s*['\"]([^'\"]+)['\"])?").allMatches(content);
              
              for (final match in dependencies) {
                final name = match.group(1)!;
                final version = match.group(2) ?? 'unknown';
                final packageKey = '$name@$version';
                
                if (!processedPackages.contains(packageKey)) {
                  processedPackages.add(packageKey);
                  components.add({
                    'type': 'library',
                    'bom-ref': 'pkg:cocoapods/$name@$version',
                    'name': name,
                    'version': version,
                    'purl': 'pkg:cocoapods/$name@$version',
                    'scope': 'required',
                    'properties': [
                      {
                        'name': 'platform',
                        'value': 'macos'
                      }
                    ]
                  });
                }
              }
            }
          }
          
          // Add plugin native dependencies
          Future<void> addPluginNativeDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final pubspecLock = File('pubspec.lock');
            if (!await pubspecLock.exists()) return;
            
            try {
              final lockContent = await pubspecLock.readAsString();
              final lockData = parsePubspecLock(lockContent);
              
              for (var package in lockData['packages']) {
                final packageName = package['name'];
                final packageVersion = package['version'];
                final packagePath = '$homeDir/.pub-cache/hosted/pub.dartlang.org/$packageName-$packageVersion';
                
                // Check Android native dependencies
                final androidGradle = File('$packagePath/android/build.gradle');
                if (await androidGradle.exists()) {
                  final content = await androidGradle.readAsString();
                  final dependencies = RegExp(r"implementation\s+['\"]([^'\"]+)['\"]").allMatches(content);
                  
                  for (final match in dependencies) {
                    final dep = match.group(1)!;
                    final parts = dep.split(':');
                    if (parts.length >= 2) {
                      final group = parts[0];
                      final name = parts[1];
                      final version = parts.length > 2 ? parts[2] : 'unknown';
                      final packageKey = '$group:$name@$version';
                      
                      if (!processedPackages.contains(packageKey)) {
                        processedPackages.add(packageKey);
                        components.add({
                          'type': 'library',
                          'bom-ref': 'pkg:maven/$group/$name@$version',
                          'name': name,
                          'version': version,
                          'purl': 'pkg:maven/$group/$name@$version',
                          'scope': 'optional',
                          'properties': [
                            {
                              'name': 'platform',
                              'value': 'android'
                            },
                            {
                              'name': 'source:plugin',
                              'value': packageName
                            }
                          ]
                        });
                      }
                    }
                  }
                }
                
                // Check iOS native dependencies
                final iosPodspec = File('$packagePath/ios/$packageName.podspec');
                if (await iosPodspec.exists()) {
                  final content = await iosPodspec.readAsString();
                  final dependencies = RegExp(r'\.dependency\s+[\'"]([^\'"]+)[\'"](?:\s*,\s*[\'"]([^\'"]+)[\'"])?').allMatches(content);
                  
                  for (final match in dependencies) {
                    final name = match.group(1)!;
                    final version = match.group(2) ?? 'unknown';
                    final packageKey = '$name@$version';
                    
                    if (!processedPackages.contains(packageKey)) {
                      processedPackages.add(packageKey);
                      components.add({
                        'type': 'library',
                        'bom-ref': 'pkg:cocoapods/$name@$version',
                        'name': name,
                        'version': version,
                        'purl': 'pkg:cocoapods/$name@$version',
                        'scope': 'optional',
                        'properties': [
                          {
                            'name': 'platform',
                            'value': 'ios'
                          },
                          {
                            'name': 'source:plugin',
                            'value': packageName
                          }
                        ]
                      });
                    }
                  }
                }
              }
            } catch (e) {
              print('Error processing plugin native dependencies: $e');
            }
          }
          
          // Parse pubspec.lock
          Map<String, dynamic> parsePubspecLock(String content) {
            final lines = content.split('\n');
            final packages = <Map<String, dynamic>>[];
            Map<String, dynamic>? currentPackage;
            
            for (final line in lines) {
              final trimmedLine = line.trim();
              if (trimmedLine.startsWith('packages:')) {
                // Start of packages section
              } else if (trimmedLine.startsWith('  ') && !trimmedLine.startsWith('    ')) {
                // Package name
                if (currentPackage != null) {
                  packages.add(currentPackage);
                }
                final packageName = trimmedLine.replaceAll(':', '');
                currentPackage = {'name': packageName};
              } else if (trimmedLine.startsWith('    ') && currentPackage != null) {
                // Package properties
                if (trimmedLine.contains('version:')) {
                  currentPackage['version'] = trimmedLine.split(':')[1].trim();
                } else if (trimmedLine.contains('source:')) {
                  currentPackage['source'] = trimmedLine.split(':')[1].trim();
                } else if (trimmedLine.contains('resolved-ref:')) {
                  currentPackage['resolved-ref'] = trimmedLine.split(':')[1].trim();
                }
              }
            }
            
            if (currentPackage != null) {
              packages.add(currentPackage);
            }
            
            return {'packages': packages};
          }
          
          // Add runtime dependencies
          Future<void> addRuntimeDependencies(List<Map<String, dynamic>> components, Set<String> processedPackages) async {
            final libDir = Directory('lib');
            if (await libDir.exists()) {
              final dartFiles = await libDir.list(recursive: true).where((entity) => 
                entity.path.endsWith('.dart')).toList();
              
              for (final file in dartFiles) {
                if (file is File) {
                  final content = await file.readAsString();
                  
                  // Detect DynamicLibrary.open() calls
                  final dynamicLibs = RegExp(r"DynamicLibrary\.open\(['\"]([^'\"]+)['\"]\)").allMatches(content);
                  for (final match in dynamicLibs) {
                    final lib = match.group(1)!;
                    final packageKey = 'runtime:$lib';
                    
                    if (!processedPackages.contains(packageKey)) {
                      processedPackages.add(packageKey);
                      components.add({
                        'type': 'library',
                        'bom-ref': 'pkg:generic/$lib',
                        'name': lib,
                        'version': 'unknown',
                        'purl': 'pkg:generic/$lib',
                        'scope': 'optional',
                        'properties': [
                          {
                            'name': 'load-type',
                            'value': 'dynamic'
                          },
                          {
                            'name': 'runtime',
                            'value': 'true'
                          }
                        ]
                      });
                    }
                  }
                }
              }
            }
          }
          
          // Enhance Git dependencies with commit hash
          Future<void> enhanceGitDependencies(Map<String, dynamic> component) async {
            if (component['purl']?.startsWith('pkg:git/') == true) {
              final lockFile = File('pubspec.lock');
              if (await lockFile.exists()) {
                try {
                  final lockContent = await lockFile.readAsString();
                  final lockData = parsePubspecLock(lockContent);
                  
                  for (var package in lockData['packages']) {
                    if (package['name'] == component['name'] && package['resolved-ref'] != null) {
                      component['properties'] ??= [];
                      component['properties'].add({
                        'name': 'git:commit',
                        'value': package['resolved-ref']
                      });
                      break;
                    }
                  }
                } catch (e) {
                  print('Error enhancing Git dependencies: $e');
                }
              }
            }
          }
          
          // Add package hashes
          Future<void> addPackageHashes(Map<String, dynamic> component) async {
            final packageName = component['name'];
            final packageVersion = component['version'];
            final homeDir = Platform.environment['HOME'] ?? Platform.environment['USERPROFILE'] ?? '';
            
            if (homeDir.isNotEmpty) {
              final cachePath = '$homeDir/.pub-cache/hosted/pub.dartlang.org/$packageName-$packageVersion';
              
              if (await Directory(cachePath).exists()) {
                try {
                  final hash = await calculateDirectoryHash(cachePath);
                  component['hashes'] = [{
                    'alg': 'SHA-256',
                    'content': hash
                  }];
                } catch (e) {
                  print('Error calculating package hash: $e');
                }
              }
            }
          }
          
          // Calculate directory hash
          Future<String> calculateDirectoryHash(String path) async {
            final result = await Process.run('find', [path, '-type', 'f', '-exec', 'sha256sum', '{}', '\\;']);
            if (result.exitCode == 0) {
              final hashes = result.stdout.toString().split('\n')
                  .where((line) => line.isNotEmpty)
                  .map((line) => line.split(' ')[0])
                  .toList();
              
              // Combine all hashes
              final combined = hashes.join('');
              final result2 = await Process.run('sh', ['-c', 'echo -n "$combined" | sha256sum']);
              if (result2.exitCode == 0) {
                return result2.stdout.toString().split(' ')[0];
              }
            }
            return 'unknown';
          }
          
          // Add build toolchain
          Future<void> addBuildToolchain(Map<String, dynamic> sbom) async {
            final tools = <Map<String, dynamic>>[];
            
            // Flutter SDK
            tools.add({
              'vendor': 'Flutter',
              'name': 'flutter',
              'version': await getFlutterVersion()
            });
            
            // Dart SDK
            tools.add({
              'vendor': 'Dart',
              'name': 'dart',
              'version': await getDartVersion()
            });
            
            // Gradle (if Android exists)
            final androidDir = Directory('android');
            if (await androidDir.exists()) {
              final gradleVersion = await getGradleVersion();
              if (gradleVersion != null) {
                tools.add({
                  'vendor': 'Gradle',
                  'name': 'gradle',
                  'version': gradleVersion
                });
              }
            }
            
            // CocoaPods (if iOS exists)
            final iosDir = Directory('ios');
            if (await iosDir.exists()) {
              final cocoapodsVersion = await getCocoaPodsVersion();
              if (cocoapodsVersion != null) {
                tools.add({
                  'vendor': 'CocoaPods',
                  'name': 'cocoapods',
                  'version': cocoapodsVersion
                });
              }
            }
            
            sbom['metadata']['tools'] = tools;
          }
          
          // Get Gradle version
          Future<String?> getGradleVersion() async {
            try {
              final result = await Process.run('gradle', ['--version']);
              if (result.exitCode == 0) {
                final lines = result.stdout.toString().split('\n');
                for (final line in lines) {
                  if (line.contains('Gradle')) {
                    final match = RegExp(r'Gradle (\d+\.\d+\.\d+)').firstMatch(line);
                    if (match != null) {
                      return match.group(1);
                    }
                  }
                }
              }
            } catch (e) {
              print('Error getting Gradle version: $e');
            }
            return null;
          }
          
          // Get CocoaPods version
          Future<String?> getCocoaPodsVersion() async {
            try {
              final result = await Process.run('pod', ['--version']);
              if (result.exitCode == 0) {
                return result.stdout.toString().trim();
              }
            } catch (e) {
              print('Error getting CocoaPods version: $e');
            }
            return null;
          }
          EOF
          
          dart run generate-enhanced-flutter-sbom.dart
          
          # Enhanced SBOM merging with deduplication
          cat > merge-enhanced-sboms.py << 'EOF'
          import json
          import sys
          from datetime import datetime
          
          def merge_sboms(syft_file, flutter_file, output_file):
              # Load SBOMs
              with open(syft_file, 'r') as f:
                  syft_sbom = json.load(f)
              
              with open(flutter_file, 'r') as f:
                  flutter_sbom = json.load(f)
              
              # Create merged SBOM
              merged_sbom = {
                  'bomFormat': 'CycloneDX',
                  'specVersion': '1.4',
                  'serialNumber': f'urn:uuid:{int(datetime.now().timestamp() * 1000)}',
                  'version': 1,
                  'metadata': {
                      'timestamp': datetime.now().isoformat(),
                      'tools': [
                          {
                              'vendor': 'Anchore',
                              'name': 'syft',
                              'version': '1.29.0'
                          },
                          {
                              'vendor': 'Flutter Security Scanner',
                              'name': 'enhanced-flutter-sbom-generator',
                              'version': '2.0.0'
                          }
                      ],
                      'component': flutter_sbom['metadata']['component']
                  },
                  'components': []
              }
              
              # Track processed components to avoid duplicates
              processed_components = set()
              merged_components = []
              
              # Process Flutter components first (more accurate for Dart ecosystem)
              for component in flutter_sbom.get('components', []):
                  purl = component.get('purl', '')
                  if purl and purl not in processed_components:
                      processed_components.add(purl)
                      merged_components.append(component)
              
              # Add Syft components that aren't already covered
              for component in syft_sbom.get('components', []):
                  purl = component.get('purl', '')
                  if purl and purl not in processed_components:
                      processed_components.add(purl)
                      merged_components.append(component)
              
              merged_sbom['components'] = merged_components
              
              # Write merged SBOM
              with open(output_file, 'w') as f:
                  json.dump(merged_sbom, f, indent=2)
              
              print(f'Successfully merged SBOMs')
              print(f'Total components: {len(merged_components)}')
              print(f'Flutter components: {len(flutter_sbom.get("components", []))}')
              print(f'Syft components: {len(syft_sbom.get("components", []))}')
              print(f'Unique components: {len(processed_components)}')
          
          if __name__ == '__main__':
              merge_sboms('syft-sbom.json', 'enhanced-flutter-sbom.json', 'enhanced-sbom.json')
          EOF
          
          python3 merge-enhanced-sboms.py
          
          # Generate comprehensive dependency analysis
          cat > generate-dependency-analysis.py << 'EOF'
          import json
          import subprocess
          import sys
          from datetime import datetime
          
          def analyze_dependencies():
              # Load enhanced SBOM
              with open('enhanced-sbom.json', 'r') as f:
                  sbom = json.load(f)
              
              analysis = {
                  'timestamp': datetime.now().isoformat(),
                  'summary': {
                      'total_components': len(sbom.get('components', [])),
                      'by_type': {},
                      'by_scope': {},
                      'by_platform': {},
                      'by_source': {}
                  },
                  'components': []
              }
              
              for component in sbom.get('components', []):
                  comp_type = component.get('type', 'unknown')
                  comp_scope = component.get('scope', 'unknown')
                  comp_purl = component.get('purl', '')
                  
                  # Count by type
                  analysis['summary']['by_type'][comp_type] = analysis['summary']['by_type'].get(comp_type, 0) + 1
                  
                  # Count by scope
                  analysis['summary']['by_scope'][comp_scope] = analysis['summary']['by_scope'].get(comp_scope, 0) + 1
                  
                  # Determine platform
                  platform = 'unknown'
                  if 'pkg:pub/' in comp_purl:
                      platform = 'dart'
                  elif 'pkg:maven/' in comp_purl:
                      platform = 'android'
                  elif 'pkg:cocoapods/' in comp_purl:
                      platform = 'ios'
                  elif 'pkg:npm/' in comp_purl:
                      platform = 'javascript'
                  elif 'pkg:pypi/' in comp_purl:
                      platform = 'python'
                  
                  analysis['summary']['by_platform'][platform] = analysis['summary']['by_platform'].get(platform, 0) + 1
                  
                  # Determine source
                  source = 'unknown'
                  if 'pkg:pub/' in comp_purl:
                      source = 'pub.dev'
                  elif 'pkg:maven/' in comp_purl:
                      source = 'maven'
                  elif 'pkg:cocoapods/' in comp_purl:
                      source = 'cocoapods'
                  elif 'pkg:git/' in comp_purl:
                      source = 'git'
                  
                  analysis['summary']['by_source'][source] = analysis['summary']['by_source'].get(source, 0) + 1
                  
                  # Add component details
                  analysis['components'].append({
                      'name': component.get('name'),
                      'version': component.get('version'),
                      'type': comp_type,
                      'scope': comp_scope,
                      'platform': platform,
                      'source': source,
                      'purl': comp_purl
                  })
              
              # Write analysis
              with open('dependency-analysis.json', 'w') as f:
                  json.dump(analysis, f, indent=2)
              
              print('Dependency analysis completed')
              print(f'Total components: {analysis["summary"]["total_components"]}')
              print('By type:', analysis['summary']['by_type'])
              print('By scope:', analysis['summary']['by_scope'])
              print('By platform:', analysis['summary']['by_platform'])
              print('By source:', analysis['summary']['by_source'])
          
          if __name__ == '__main__':
              analyze_dependencies()
          EOF
          
          python3 generate-dependency-analysis.py
            
          # Generate dependency tree
          flutter pub deps --style=tree > dependency-tree.txt
          
          # Generate outdated report
          flutter pub outdated --json > outdated.json || echo '{}' > outdated.json
          
          # Generate comprehensive SBOM validation and completeness check
          cat > validate-sbom.py << 'EOF'
          import json
          import sys
          import os
          import yaml
          from datetime import datetime
          
          def validate_sbom_completeness(sbom, project_root):
              """Validate SBOM completeness"""
              issues = []
              
              # 1. Check if all dependencies from pubspec.yaml are included
              try:
                  with open(f'{project_root}/pubspec.yaml', 'r') as f:
                      pubspec_content = f.read()
                  
                  pubspec_deps = parse_pubspec_yaml(pubspec_content)
                  sbom_packages = {c['name'] for c in sbom.get('components', [])}
                  
                  missing_deps = set(pubspec_deps.get('dependencies', {}).keys()) - sbom_packages
                  if missing_deps:
                      issues.append(f"Missing dependencies: {missing_deps}")
              except Exception as e:
                  issues.append(f"Error parsing pubspec.yaml: {e}")
              
              # 2. Check if native platform dependencies are included
              if os.path.exists(f'{project_root}/android'):
                  android_deps = [c for c in sbom['components'] if 'maven' in c.get('purl', '')]
                  if not android_deps:
                      issues.append("No Android dependencies found")
              
              if os.path.exists(f'{project_root}/ios'):
                  ios_deps = [c for c in sbom['components'] if 'cocoapods' in c.get('purl', '')]
                  if not ios_deps:
                      issues.append("No iOS dependencies found")
              
              # 3. Check if transitive dependencies are included
              direct_deps = [c for c in sbom['components'] if c.get('scope') == 'required']
              transitive_deps = [c for c in sbom['components'] if c.get('scope') == 'optional']
              if len(transitive_deps) == 0 and len(direct_deps) > 5:
                  issues.append("Suspicious: No transitive dependencies found")
              
              # 4. Check if build toolchain information is included
              tools = sbom.get('metadata', {}).get('tools', [])
              if not tools:
                  issues.append("No build toolchain information found")
              
              # 5. Check if runtime dependencies are included
              runtime_deps = [c for c in sbom['components'] if c.get('properties') and 
                            any(p.get('name') == 'runtime' for p in c['properties'])]
              if not runtime_deps:
                  issues.append("No runtime dependencies detected")
              
              return issues
          
          def parse_pubspec_yaml(content):
              """Parse pubspec.yaml file"""
              lines = content.split('\n')
              info = {}
              current_section = ''
              
              for line in lines:
                  trimmed_line = line.strip()
                  if trimmed_line.startswith('dependencies:'):
                      current_section = 'dependencies'
                      info[current_section] = {}
                  elif trimmed_line.startswith('dev_dependencies:'):
                      current_section = 'dev_dependencies'
                      info[current_section] = {}
                  elif current_section and trimmed_line.startswith('  ') and ':' in trimmed_line:
                      parts = trimmed_line.split(':')
                      if len(parts) >= 2:
                          dep_name = parts[0].strip()
                          dep_version = parts[1].strip()
                          info[current_section][dep_name] = dep_version
              
              return info
          
          def validate_sbom():
              try:
                  with open('enhanced-sbom.json', 'r') as f:
                      sbom = json.load(f)
                  
                  validation = {
                      'valid': True,
                      'errors': [],
                      'warnings': [],
                      'completeness_issues': [],
                      'coverage': {
                          'total_components': len(sbom.get('components', [])),
                          'with_license': 0,
                          'with_description': 0,
                          'with_purl': 0,
                          'with_hash': 0,
                          'by_platform': {},
                          'by_source': {},
                          'by_scope': {}
                      }
                  }
                  
                  # Analyze components
                  for component in sbom.get('components', []):
                      if component.get('licenses'):
                          validation['coverage']['with_license'] += 1
                      if component.get('description'):
                          validation['coverage']['with_description'] += 1
                      if component.get('purl'):
                          validation['coverage']['with_purl'] += 1
                      if component.get('hashes'):
                          validation['coverage']['with_hash'] += 1
                      
                      # Count by platform
                      platform = 'unknown'
                      purl = component.get('purl', '')
                      if 'pkg:pub/' in purl:
                          platform = 'dart'
                      elif 'pkg:maven/' in purl:
                          platform = 'android'
                      elif 'pkg:cocoapods/' in purl:
                          platform = 'ios'
                      elif 'pkg:cmake/' in purl:
                          platform = 'native'
                      elif 'pkg:cdn/' in purl:
                          platform = 'web'
                      
                      validation['coverage']['by_platform'][platform] = validation['coverage']['by_platform'].get(platform, 0) + 1
                      
                      # Count by scope
                      scope = component.get('scope', 'unknown')
                      validation['coverage']['by_scope'][scope] = validation['coverage']['by_scope'].get(scope, 0) + 1
                      
                      # Count by source
                      source = 'unknown'
                      if 'pkg:pub/' in purl:
                          source = 'pub.dev'
                      elif 'pkg:maven/' in purl:
                          source = 'maven'
                      elif 'pkg:cocoapods/' in purl:
                          source = 'cocoapods'
                      elif 'pkg:git/' in purl:
                          source = 'git'
                      elif 'pkg:cdn/' in purl:
                          source = 'cdn'
                      
                      validation['coverage']['by_source'][source] = validation['coverage']['by_source'].get(source, 0) + 1
                  
                  # Calculate coverage percentages
                  total = validation['coverage']['total_components']
                  if total > 0:
                      validation['coverage']['license_coverage'] = validation['coverage']['with_license'] / total * 100
                      validation['coverage']['description_coverage'] = validation['coverage']['with_description'] / total * 100
                      validation['coverage']['purl_coverage'] = validation['coverage']['with_purl'] / total * 100
                      validation['coverage']['hash_coverage'] = validation['coverage']['with_hash'] / total * 100
                  
                  # Check for required fields
                  if not sbom.get('metadata', {}).get('component', {}).get('name'):
                      validation['errors'].append('Missing root component name')
                      validation['valid'] = False
                  
                  if not sbom.get('components'):
                      validation['warnings'].append('No components found in SBOM')
                  
                  # Check completeness
                  validation['completeness_issues'] = validate_sbom_completeness(sbom, '.')
                  
                  # Add completeness issues to errors/warnings
                  for issue in validation['completeness_issues']:
                      if 'Missing' in issue or 'No' in issue:
                          validation['errors'].append(issue)
                          validation['valid'] = False
                      else:
                          validation['warnings'].append(issue)
                  
                  # Generate comprehensive report
                  report = {
                      'timestamp': datetime.now().isoformat(),
                      'validation': validation,
                      'summary': {
                          'total_components': validation['coverage']['total_components'],
                          'platforms_covered': len(validation['coverage']['by_platform']),
                          'sources_covered': len(validation['coverage']['by_source']),
                          'scopes_covered': len(validation['coverage']['by_scope']),
                          'overall_coverage_score': calculate_coverage_score(validation)
                      }
                  }
                  
                  with open('sbom-validation.json', 'w') as f:
                      json.dump(report, f, indent=2)
                  
                  print('Comprehensive SBOM validation completed')
                  print(f'Valid: {validation["valid"]}')
                  print(f'Errors: {len(validation["errors"])}')
                  print(f'Warnings: {len(validation["warnings"])}')
                  print(f'Completeness Issues: {len(validation["completeness_issues"])}')
                  print(f'Coverage Score: {report["summary"]["overall_coverage_score"]}/100')
                  print(f'Platforms: {validation["coverage"]["by_platform"]}')
                  print(f'Sources: {validation["coverage"]["by_source"]}')
                  
                  return validation['valid']
                  
              except Exception as e:
                  print(f'SBOM validation failed: {e}')
                  return False
          
          def calculate_coverage_score(validation):
              """Calculate coverage score"""
              score = 100
              
              # Base score
              coverage = validation['coverage']
              total = coverage['total_components']
              
              if total > 0:
                  # Metadata completeness
                  score -= (100 - coverage.get('purl_coverage', 0)) * 0.3
                  score -= (100 - coverage.get('license_coverage', 0)) * 0.2
                  score -= (100 - coverage.get('description_coverage', 0)) * 0.1
                  score -= (100 - coverage.get('hash_coverage', 0)) * 0.1
                  
                  # Platform coverage
                  platforms = coverage['by_platform']
                  if 'dart' not in platforms:
                      score -= 20
                  if 'android' not in platforms and 'ios' not in platforms:
                      score -= 15
                  
                  # Scope coverage
                  scopes = coverage['by_scope']
                  if 'required' not in scopes:
                      score -= 10
                  if 'optional' not in scopes:
                      score -= 5
                  
                  # Error and warning penalties
                  score -= len(validation['errors']) * 10
                  score -= len(validation['warnings']) * 5
                  
                  # Completeness check penalties
                  score -= len(validation['completeness_issues']) * 8
              
              return max(0, int(score))
          
          if __name__ == '__main__':
              success = validate_sbom()
              sys.exit(0 if success else 1)
          EOF
          
          python3 validate-sbom.py
          
      - name: Upload SBOM artifacts
        uses: actions/upload-artifact@v4
        with:
          name: software-bill-of-materials
          path: |
            enhanced-sbom.json
            syft-sbom.json
            enhanced-flutter-sbom.json
            dependency-analysis.json
            sbom-validation.json
            dependency-tree.txt
            outdated.json
          retention-days: 90

  # Dependency vulnerability scanning
  dependency-vulnerability-scan:
    needs: [generate-sbom]
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - uses: actions/checkout@v4
        
      - name: Download SBOM
        uses: actions/download-artifact@v4
        with:
          name: software-bill-of-materials
          
      - name: Vulnerability scanning suite
        run: |
          # Install tools
          curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin
          
          # Grype scan with enhanced SBOM
          grype sbom:enhanced-sbom.json --output sarif --file grype-results.sarif || {
            EXIT_CODE=$?
            echo "Grype scan completed with exit code: $EXIT_CODE"
            if [ ! -f "grype-results.sarif" ]; then
              echo '{"version": "2.1.0", "runs": []}' > grype-results.sarif
            fi
          }
          
          # OSV Scanner with enhanced SBOM
          wget -q https://github.com/google/osv-scanner/releases/latest/download/osv-scanner_linux_amd64
          chmod +x osv-scanner_linux_amd64
          ./osv-scanner_linux_amd64 --sbom=enhanced-sbom.json --format=json > osv-results.json || {
            EXIT_CODE=$?
            echo "OSV Scanner completed with exit code: $EXIT_CODE"
            if [ ! -f "osv-results.json" ] || [ ! -s "osv-results.json" ]; then
              echo '{"results": []}' > osv-results.json
            fi
          }
          
          # Generate enhanced vulnerability report
          cat > generate-enhanced-vulnerability-report.py << 'EOF'
          import json
          import sys
          from datetime import datetime
          
          def generate_enhanced_vulnerability_report():
              try:
                  # Load SBOM and analysis
                  with open('enhanced-sbom.json', 'r') as f:
                      sbom = json.load(f)
                  
                  with open('dependency-analysis.json', 'r') as f:
                      analysis = json.load(f)
                  
                  # Load vulnerability results
                  grype_results = {}
                  osv_results = {}
                  
                  try:
                      with open('grype-results.sarif', 'r') as f:
                          grype_results = json.load(f)
                  except:
                      grype_results = {'runs': []}
                  
                  try:
                      with open('osv-results.json', 'r') as f:
                          osv_results = json.load(f)
                  except:
                      osv_results = {'results': []}
                  
                  # Generate comprehensive vulnerability report
                  report = {
                      'timestamp': datetime.now().isoformat(),
                      'sbom_summary': {
                          'total_components': len(sbom.get('components', [])),
                          'by_platform': analysis['summary']['by_platform'],
                          'by_source': analysis['summary']['by_source']
                      },
                      'vulnerability_summary': {
                          'grype_findings': 0,
                          'osv_findings': 0,
                          'total_vulnerabilities': 0,
                          'by_severity': {
                              'critical': 0,
                              'high': 0,
                              'medium': 0,
                              'low': 0
                          }
                      },
                      'coverage_analysis': {
                          'dart_packages': analysis['summary']['by_platform'].get('dart', 0),
                          'android_dependencies': analysis['summary']['by_platform'].get('android', 0),
                          'ios_dependencies': analysis['summary']['by_platform'].get('ios', 0),
                          'build_tools': analysis['summary']['by_platform'].get('unknown', 0)
                      },
                      'recommendations': []
                  }
                  
                  # Count Grype findings
                  for run in grype_results.get('runs', []):
                      for result in run.get('results', []):
                          report['vulnerability_summary']['grype_findings'] += 1
                          report['vulnerability_summary']['total_vulnerabilities'] += 1
                          
                          # Count by severity
                          severity = result.get('level', 'medium').lower()
                          if severity in report['vulnerability_summary']['by_severity']:
                              report['vulnerability_summary']['by_severity'][severity] += 1
                  
                  # Count OSV findings
                  for result in osv_results.get('results', []):
                      report['vulnerability_summary']['osv_findings'] += 1
                      report['vulnerability_summary']['total_vulnerabilities'] += 1
                      
                      # Count by severity
                      severity = result.get('vulnerability', {}).get('severity', 'medium').lower()
                      if severity in report['vulnerability_summary']['by_severity']:
                          report['vulnerability_summary']['by_severity'][severity] += 1
                  
                  # Generate recommendations
                  if report['vulnerability_summary']['by_severity']['critical'] > 0:
                      report['recommendations'].append({
                          'priority': 'CRITICAL',
                          'message': f"Fix {report['vulnerability_summary']['by_severity']['critical']} critical vulnerabilities immediately"
                      })
                  
                  if report['vulnerability_summary']['by_severity']['high'] > 0:
                      report['recommendations'].append({
                          'priority': 'HIGH',
                          'message': f"Address {report['vulnerability_summary']['by_severity']['high']} high severity vulnerabilities"
                      })
                  
                  # Coverage recommendations
                  coverage = report['coverage_analysis']
                  if coverage['android_dependencies'] > 0:
                      report['recommendations'].append({
                          'priority': 'MEDIUM',
                          'message': f"Review {coverage['android_dependencies']} Android native dependencies for security"
                      })
                  
                  if coverage['ios_dependencies'] > 0:
                      report['recommendations'].append({
                          'priority': 'MEDIUM',
                          'message': f"Review {coverage['ios_dependencies']} iOS native dependencies for security"
                      })
                  
                  # Write enhanced report
                  with open('enhanced-vulnerability-report.json', 'w') as f:
                      json.dump(report, f, indent=2)
                  
                  print('Enhanced vulnerability report generated')
                  print(f'Total components scanned: {report["sbom_summary"]["total_components"]}')
                  print(f'Total vulnerabilities found: {report["vulnerability_summary"]["total_vulnerabilities"]}')
                  print(f'Coverage: {report["coverage_analysis"]}')
                  
                  return True
                  
              except Exception as e:
                  print(f'Error generating enhanced vulnerability report: {e}')
                  return False
          
          if __name__ == '__main__':
              success = generate_enhanced_vulnerability_report()
              sys.exit(0 if success else 1)
          EOF
          
          python3 generate-enhanced-vulnerability-report.py
          
      - name: Trivy filesystem scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: ${{ env.TRIVY_SEVERITY }}
          
      - name: Upload vulnerability scan results
        uses: actions/upload-artifact@v4
        with:
          name: vulnerability-scan-results
          path: |
            grype-results.sarif
            trivy-results.sarif
            osv-results.json
            enhanced-vulnerability-report.json
          retention-days: 30

  # Static Application Security Testing (SAST)
  sast-scan:
    needs: [pre-check, setup-and-cache]
    if: needs.pre-check.outputs.should_run == 'true'
    strategy:
      fail-fast: false
      max-parallel: 3
      matrix:
        include:
          - tool: semgrep
            config: 'p/security-audit p/owasp-top-ten auto'
            timeout: 10
          - tool: flutter-analyzer
            timeout: 8
          - tool: dcm
            timeout: 10
    
    runs-on: ubuntu-latest
    timeout-minutes: ${{ matrix.timeout }}
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'
          
      - name: Get dependencies
        run: flutter pub get
          
      - name: Install security tools
        run: |
          if [[ "${{ matrix.tool }}" == "semgrep" ]]; then
            pipx install "semgrep==${{ env.SEMGREP_VERSION }}"
          elif [[ "${{ matrix.tool }}" == "dcm" ]]; then
            dart pub global activate dart_code_metrics
          fi
          
      - name: Run security analysis
        run: |
          case "${{ matrix.tool }}" in
            semgrep)
              semgrep --config=${{ matrix.config }} --sarif > ${{ matrix.tool }}-results.sarif || {
                echo "Semgrep found issues"
                if [ ! -f "${{ matrix.tool }}-results.sarif" ]; then
                  echo '{"version": "2.1.0", "runs": []}' > ${{ matrix.tool }}-results.sarif
                fi
              }
              ;;
            flutter-analyzer)
              dart analyze --format=machine > ${{ matrix.tool }}-results.txt || {
                echo "Flutter analyzer found issues"
                touch ${{ matrix.tool }}-results.txt
              }
              # Convert to SARIF format using a simple approach
              echo 'import "dart:io"; import "dart:convert"; void main() async { final input = File("flutter-analyzer-results.txt"); if (!await input.exists()) { final emptySarif = {"version": "2.1.0", "runs": [{"tool": {"driver": {"name": "flutter-analyzer", "version": "1.0.0"}}, "results": []}]}; await File("flutter-analyzer-results.sarif").writeAsString(JsonEncoder.withIndent("  ").convert(emptySarif)); return; } final lines = await input.readAsLines(); final results = []; for (final line in lines) { if (line.contains("bullet")) { final parts = line.split("bullet"); if (parts.length >= 3) { final location = parts[0].trim(); final severity = parts[1].trim(); final message = parts[2].trim(); final locationParts = location.split(":"); if (locationParts.length >= 2) { results.add({"ruleId": "flutter-analyzer", "level": severity.toLowerCase() == "error" ? "error" : "warning", "message": {"text": message}, "locations": [{"physicalLocation": {"artifactLocation": {"uri": locationParts[0]}, "region": {"startLine": int.tryParse(locationParts[1]) ?? 1}}}]}); } } } } final sarif = {"version": "2.1.0", "runs": [{"tool": {"driver": {"name": "flutter-analyzer", "version": "1.0.0"}}, "results": results}]}; await File("flutter-analyzer-results.sarif").writeAsString(JsonEncoder.withIndent("  ").convert(sarif)); }' > convert-to-sarif.dart
              
              dart run convert-to-sarif.dart
              ;;
            dcm)
              dart pub global run dart_code_metrics:metrics analyze lib --reporter=sarif:${{ matrix.tool }}-results.sarif || {
                echo "DCM analysis found issues"
                if [ ! -f "${{ matrix.tool }}-results.sarif" ]; then
                  echo '{"version": "2.1.0", "runs": []}' > ${{ matrix.tool }}-results.sarif
                fi
              }
              ;;
          esac
          
      - name: Upload SAST results
        uses: actions/upload-artifact@v4
        with:
          name: sast-${{ matrix.tool }}-results
          path: |
            ${{ matrix.tool }}-results.sarif
            ${{ matrix.tool }}-results.txt
          retention-days: 30

  # Supply chain security
  supply-chain-security:
    needs: [generate-sbom]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          
      - name: Download SBOM
        uses: actions/download-artifact@v4
        with:
          name: software-bill-of-materials
          
      - name: License compliance check
        run: |
          cat > check-licenses.py << 'EOF'
          import json
          import sys
          from datetime import datetime
          
          PROHIBITED_LICENSES = [
              'GPL', 'GPL-2.0', 'GPL-3.0', 
              'AGPL', 'AGPL-3.0',
              'LGPL', 'LGPL-2.1', 'LGPL-3.0',
              'SSPL', 'OSL'
          ]
          WARNING_LICENSES = [
              'MPL', 'MPL-2.0',
              'CC-BY-SA', 'CC-BY-NC'
          ]
          
          def check_licenses(sbom_file):
              try:
              with open(sbom_file, 'r') as f:
                  sbom = json.load(f)
              report = {
                  'timestamp': datetime.now().isoformat(),
                  'compliant': True,
                  'violations': [],
                  'warnings': [],
                  'summary': {
                      'total_packages': 0,
                      'prohibited_licenses': 0,
                      'warning_licenses': 0
                  }
              }
              components = sbom.get('components', [])
              report['summary']['total_packages'] = len(components)
              for component in components:
                  name = component.get('name', 'unknown')
                  version = component.get('version', 'unknown')
                  licenses = component.get('licenses', [])
                  for license_info in licenses:
                      license_id = license_info.get('license', {}).get('id', '')
                      if any(prohibited in license_id.upper() for prohibited in PROHIBITED_LICENSES):
                          report['violations'].append({
                              'package': name,
                              'version': version,
                              'license': license_id,
                              'severity': 'HIGH'
                          })
                          report['summary']['prohibited_licenses'] += 1
                          report['compliant'] = False
                      elif any(warning in license_id.upper() for warning in WARNING_LICENSES):
                          report['warnings'].append({
                              'package': name,
                              'version': version,
                              'license': license_id,
                              'severity': 'MEDIUM'
                          })
                          report['summary']['warning_licenses'] += 1
              with open('license-compliance-report.json', 'w') as f:
                  json.dump(report, f, indent=2)
              print(f"License compliance check completed")
              print(f"Total packages: {report['summary']['total_packages']}")
              print(f"Prohibited licenses: {report['summary']['prohibited_licenses']}")
              print(f"Warning licenses: {report['summary']['warning_licenses']}")
              return report['compliant']
              except Exception as e:
                  print(f'Error processing license compliance: {e}', file=sys.stderr)
                  import traceback
                  traceback.print_exc()
                  return False
          
          if __name__ == '__main__':
              try:
                  compliant = check_licenses('enhanced-sbom.json')
              sys.exit(0 if compliant else 1)
              except Exception as e:
                  print(f'Fatal error: {e}', file=sys.stderr)
                  sys.exit(2)
          EOF
          python3 check-licenses.py || echo "License compliance issues found"
          
      - name: Secret scanning
        run: |
                    # Install TruffleHog using official installation script
                    curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
                    
                    # Verify installation
                    trufflehog --version
                    
                    # Run scan
                    trufflehog filesystem . --json --no-verification > secret-scan-results.json || {
                      EXIT_CODE=$?
                      echo "TruffleHog scan completed with exit code: $EXIT_CODE"
                              }
                              
                              # Ensure output file exists and is valid
                              if [ ! -f "secret-scan-results.json" ] || [ ! -s "secret-scan-results.json" ]; then
                                echo '[]' > secret-scan-results.json
                              fi
                              
                              # Count discovered secrets
                    if [ -f "secret-scan-results.json" ]; then
                                # TruffleHog v3 output is JSON array
                                SECRET_COUNT=$(wc -l < secret-scan-results.json 2>/dev/null || echo "0")
                      echo "Found $SECRET_COUNT potential secrets"
                      if [ "$SECRET_COUNT" -gt 0 ]; then
                        echo "::warning::Found $SECRET_COUNT potential secrets in the codebase"
                      fi
                    fi
          
      - name: Dependency integrity check
        run: |
          echo "Checking dependency integrity..."
          
          # Check for git dependencies
          if grep -q '"source": "git"' sbom.json; then
            echo "::warning::Found git dependencies - these should be reviewed for security"
            
            # Extract git dependencies
            cat > extract-git-deps.py << 'EOF'
          import json
          
          with open('sbom.json', 'r') as f:
              sbom = json.load(f)
          
          git_deps = []
          for component in sbom.get('components', []):
              if component.get('purl', '').startswith('pkg:git/'):
                  git_deps.append({
                      'name': component.get('name'),
                      'version': component.get('version'),
                      'purl': component.get('purl')
                  })
          
          if git_deps:
              print("Git dependencies found:")
              for dep in git_deps:
                  print(f"  - {dep['name']} @ {dep['version']}")
          
          with open('git-dependencies.json', 'w') as f:
              json.dump({'git_dependencies': git_deps}, f, indent=2)
          EOF
          
            python3 extract-git-deps.py
          fi
          
      - name: Upload supply chain results
        uses: actions/upload-artifact@v4
        with:
          name: supply-chain-security-results
          path: |
            license-compliance-report.json
            secret-scan-results.json
            git-dependencies.json
          retention-days: 30

  # Merge and upload SARIF reports
  upload-security-results:
    runs-on: ubuntu-latest
    needs: [sast-scan, dependency-vulnerability-scan]
    if: always() && (needs.sast-scan.result != 'skipped' || needs.dependency-vulnerability-scan.result != 'skipped')
    permissions:
      security-events: write
      
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all scan results
        uses: actions/download-artifact@v4
        with:
          path: scan-results/
          
      - name: Merge SARIF files
        run: |
          cat > merge-sarif.py << 'EOF'
          import json
          import glob
          import os
          from datetime import datetime
          
          def merge_sarif_files():
              merged_sarif = {
                  "version": "2.1.0",
                  "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
                  "runs": []
              }
              
              stats = {
                  "files_processed": 0,
                  "runs_merged": 0,
                  "total_results": 0
              }
              
              # Find all SARIF files
              sarif_files = glob.glob("scan-results/**/*.sarif", recursive=True)
              print(f"Found {len(sarif_files)} SARIF files to merge")
              
              for sarif_file in sarif_files:
                  try:
                      with open(sarif_file, 'r') as f:
                          sarif_data = json.load(f)
                      
                      if 'runs' in sarif_data:
                          for run in sarif_data['runs']:
                              # Count results
                              if 'results' in run:
                                  stats['total_results'] += len(run['results'])
                              
                              # Add metadata
                              if 'properties' not in run:
                                  run['properties'] = {}
                              run['properties']['sourceFile'] = os.path.basename(sarif_file)
                              
                              merged_sarif['runs'].append(run)
                              stats['runs_merged'] += 1
                          
                      stats['files_processed'] += 1
                      print(f" Merged: {sarif_file}")
                  except Exception as e:
                      print(f" Error processing {sarif_file}: {e}")
              
              # Add merge metadata
              merged_sarif['properties'] = {
                  'mergeStats': stats,
                  'mergeTimestamp': datetime.now().isoformat()
              }
              
              # Save merged file
              with open('merged-security-results.sarif', 'w') as f:
                  json.dump(merged_sarif, f, indent=2)
              
              print(f"\n=== Merge Summary ===")
              print(f"Files processed: {stats['files_processed']}")
              print(f"Runs merged: {stats['runs_merged']}")
              print(f"Total results: {stats['total_results']}")
              
              return stats['runs_merged'] > 0
          
          if __name__ == '__main__':
              success = merge_sarif_files()
              if not success:
                  print("Warning: No SARIF runs were merged")
          EOF
          
          python3 merge-sarif.py
          
      - name: Upload to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: merged-security-results.sarif
          category: flutter-security
        continue-on-error: true
        
      - name: Upload merged SARIF as artifact
        uses: actions/upload-artifact@v4
        with:
          name: merged-security-sarif
          path: merged-security-results.sarif
          retention-days: 90

  # Generate unified security report
  unified-security-report:
    runs-on: ubuntu-latest
    needs: [dependency-vulnerability-scan, sast-scan, supply-chain-security]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Generate unified security report
        run: |
          cat > generate-unified-report.py << 'EOF'
          import json
          import os
          import glob
          from datetime import datetime
          
          def load_json_file(filepath):
              try:
                  with open(filepath, 'r') as f:
                      return json.load(f)
              except:
                  return None
          
          def count_sarif_issues(sarif_data):
              issues = {'error': 0, 'warning': 0, 'note': 0}
              if sarif_data and 'runs' in sarif_data:
                  for run in sarif_data['runs']:
                      for result in run.get('results', []):
                          level = result.get('level', 'warning')
                          issues[level] = issues.get(level, 0) + 1
              return issues
          
          # Initialize unified report
          unified_report = {
              'timestamp': datetime.now().isoformat(),
              'summary': {
                  'scan_type': os.getenv('SCAN_TYPE', 'full'),
                  'total_vulnerabilities': 0,
                  'critical_vulnerabilities': 0,
                  'high_vulnerabilities': 0,
                  'medium_vulnerabilities': 0,
                  'low_vulnerabilities': 0,
                  'sast_issues': 0,
                  'dependency_issues': 0,
                  'supply_chain_issues': 0,
                  'license_compliant': True,
                  'secrets_found': 0,
                  'overall_score': 100
              },
              'details': {
                  'sast_analysis': {},
                  'dependency_scanning': {},
                  'supply_chain': {},
                  'recommendations': []
              }
          }# Process SAST reports
          sast_files = glob.glob('artifacts/sast-*-results/*.sarif')
          for sarif_file in sast_files:
              data = load_json_file(sarif_file)
              if data:
                  tool_name = os.path.basename(sarif_file).replace('-results.sarif', '')
                  unified_report['details']['sast_analysis'][tool_name] = data
                  issues = count_sarif_issues(data)
                  unified_report['summary']['sast_issues'] += sum(issues.values())
          
          # Process vulnerability scan results
          vuln_files = glob.glob('artifacts/vulnerability-scan-results/*.sarif')
          for sarif_file in vuln_files:
              data = load_json_file(sarif_file)
              if data:
                  tool_name = os.path.basename(sarif_file).replace('-results.sarif', '')
                  unified_report['details']['dependency_scanning'][tool_name] = data
                  issues = count_sarif_issues(data)
                  unified_report['summary']['critical_vulnerabilities'] += issues.get('error', 0)
                  unified_report['summary']['high_vulnerabilities'] += issues.get('warning', 0)
                  unified_report['summary']['medium_vulnerabilities'] += issues.get('note', 0)
          
          # Process OSV results
          osv_file = 'artifacts/vulnerability-scan-results/osv-results.json'
          if os.path.exists(osv_file):
              osv_data = load_json_file(osv_file)
              if osv_data and 'results' in osv_data:
                  unified_report['details']['dependency_scanning']['osv'] = osv_data
                  for result in osv_data.get('results', []):
                      severity = result.get('vulnerability', {}).get('severity', 'MEDIUM')
                      if severity == 'CRITICAL':
                          unified_report['summary']['critical_vulnerabilities'] += 1
                      elif severity == 'HIGH':
                          unified_report['summary']['high_vulnerabilities'] += 1
                      else:
                          unified_report['summary']['medium_vulnerabilities'] += 1
          
          # Process supply chain results
          license_file = 'artifacts/supply-chain-security-results/license-compliance-report.json'
          if os.path.exists(license_file):
              license_data = load_json_file(license_file)
              if license_data:
                  unified_report['details']['supply_chain']['license_compliance'] = license_data
                  unified_report['summary']['license_compliant'] = license_data.get('compliant', True)
                  if license_data.get('violations'):
                      unified_report['summary']['supply_chain_issues'] += len(license_data['violations'])
          
          secret_file = 'artifacts/supply-chain-security-results/secret-scan-results.json'
          if os.path.exists(secret_file):
              secret_data = load_json_file(secret_file)
              if secret_data:
                  # Count secrets (each line with SourceMetadata is a finding)
                  with open(secret_file, 'r') as f:
                      content = f.read()
                      secret_count = content.count('"SourceMetadata"')
                  unified_report['summary']['secrets_found'] = secret_count
                  unified_report['summary']['supply_chain_issues'] += secret_count
          
          # Calculate total vulnerabilities
          unified_report['summary']['total_vulnerabilities'] = (
              unified_report['summary']['critical_vulnerabilities'] +
              unified_report['summary']['high_vulnerabilities'] +
              unified_report['summary']['medium_vulnerabilities'] +
              unified_report['summary']['low_vulnerabilities']
          )
          
          unified_report['summary']['dependency_issues'] = unified_report['summary']['total_vulnerabilities']
          
          # Calculate overall score
          score_deductions = (
              unified_report['summary']['critical_vulnerabilities'] * 20 +
              unified_report['summary']['high_vulnerabilities'] * 10 +
              unified_report['summary']['medium_vulnerabilities'] * 5 +
              unified_report['summary']['low_vulnerabilities'] * 2 +
              unified_report['summary']['sast_issues'] * 3 +
              unified_report['summary']['supply_chain_issues'] * 5 +
              unified_report['summary']['secrets_found'] * 15
          )
          
          unified_report['summary']['overall_score'] = max(0, 100 - score_deductions)
          
          # Generate recommendations
          if unified_report['summary']['critical_vulnerabilities'] > 0:
              unified_report['details']['recommendations'].append({
                  'priority': 'CRITICAL',
                  'message': f"Fix {unified_report['summary']['critical_vulnerabilities']} critical vulnerabilities immediately"
              })
          
          if unified_report['summary']['high_vulnerabilities'] > 0:
              unified_report['details']['recommendations'].append({
                  'priority': 'HIGH',
                  'message': f"Address {unified_report['summary']['high_vulnerabilities']} high severity vulnerabilities"
              })
          
          if unified_report['summary']['secrets_found'] > 0:
              unified_report['details']['recommendations'].append({
                  'priority': 'CRITICAL',
                  'message': f"Remove {unified_report['summary']['secrets_found']} exposed secrets from the codebase"
              })
          
          if not unified_report['summary']['license_compliant']:
              unified_report['details']['recommendations'].append({
                  'priority': 'HIGH',
                  'message': 'Review and resolve license compliance issues'
              })
          
          if unified_report['summary']['sast_issues'] > 20:
              unified_report['details']['recommendations'].append({
                  'priority': 'MEDIUM',
                  'message': 'Improve code quality to reduce static analysis findings'
              })
          
          # Check for outdated dependencies
          outdated_file = 'artifacts/software-bill-of-materials/outdated.json'
          if os.path.exists(outdated_file):
              outdated_data = load_json_file(outdated_file)
              if outdated_data and 'packages' in outdated_data:
                  outdated_count = len(outdated_data['packages'])
                  if outdated_count > 10:
                      unified_report['details']['recommendations'].append({
                          'priority': 'LOW',
                          'message': f'Update {outdated_count} outdated dependencies'
                      })
          
          # Write unified report
          with open('unified-security-report.json', 'w') as f:
              json.dump(unified_report, f, indent=2)
          
          # Generate summary output
          print("=== Security Scan Summary ===")
          print(f"Overall Security Score: {unified_report['summary']['overall_score']}/100")
          print(f"Total Vulnerabilities: {unified_report['summary']['total_vulnerabilities']}")
          print(f"  - Critical: {unified_report['summary']['critical_vulnerabilities']}")
          print(f"  - High: {unified_report['summary']['high_vulnerabilities']}")
          print(f"  - Medium: {unified_report['summary']['medium_vulnerabilities']}")
          print(f"SAST Issues: {unified_report['summary']['sast_issues']}")
          print(f"Supply Chain Issues: {unified_report['summary']['supply_chain_issues']}")
          print(f"Secrets Found: {unified_report['summary']['secrets_found']}")
          print(f"License Compliant: {unified_report['summary']['license_compliant']}")
          
          # Exit with error if critical issues found
          if (unified_report['summary']['critical_vulnerabilities'] > 0 or 
              unified_report['summary']['secrets_found'] > 0 or
              unified_report['summary']['overall_score'] < 70):
              print("\n Security scan failed due to critical issues")
              exit(1)
          else:
              print("\n Security scan passed")
          EOF
          
          python3 generate-unified-report.py || echo "Security issues found"
          
      - name: Generate markdown report
        run: |
          cat > generate-markdown-report.py << 'EOF'
          import json
          from datetime import datetime
          
          with open('unified-security-report.json', 'r') as f:
              report = json.load(f)
          
          summary = report['summary']
          
          markdown = f"""# Flutter Security Scan Report
          
          Generated: {report['timestamp']}
          
          ## Summary
          
          **Overall Security Score: {summary['overall_score']}/100** {'' if summary['overall_score'] >= 70 else ''}
          
          ### Vulnerability Summary
          - **Total Vulnerabilities**: {summary['total_vulnerabilities']}
            -  Critical: {summary['critical_vulnerabilities']}
            -  High: {summary['high_vulnerabilities']}
            -  Medium: {summary['medium_vulnerabilities']}
            -  Low: {summary['low_vulnerabilities']}
          
          ### Other Findings
          - **SAST Issues**: {summary['sast_issues']}
          - **Supply Chain Issues**: {summary['supply_chain_issues']}
          - **Secrets Found**: {summary['secrets_found']} {'' if summary['secrets_found'] > 0 else ''}
          - **License Compliance**: {' Compliant' if summary['license_compliant'] else ' Non-compliant'}
          
          ## Recommendations
          """
          
          for rec in report['details']['recommendations']:
              emoji = {'CRITICAL': '', 'HIGH': '', 'MEDIUM': '', 'LOW': ''}.get(rec['priority'], '')
              markdown += f"\n- {emoji} **{rec['priority']}**: {rec['message']}"
          
          if not report['details']['recommendations']:
              markdown += "\n\n No critical recommendations at this time."
          
          markdown += "\n\n---\n*Report generated by Flutter Security Pipeline*"
          
          with open('security-report.md', 'w') as f:
              f.write(markdown)
          
          # Also create a summary for PR comments
          pr_summary = f"""###  Security Scan Results
          
          **Score**: {summary['overall_score']}/100 {'' if summary['overall_score'] >= 70 else ''}
          
          | Type | Count |
          |------|-------|
          |  Critical | {summary['critical_vulnerabilities']} |
          |  High | {summary['high_vulnerabilities']} |
          |  Medium | {summary['medium_vulnerabilities']} |
          |  SAST | {summary['sast_issues']} |
          | Secrets | {summary['secrets_found']} |
          
          [View Full Report](https://github.com/${{github.repository}}/actions/runs/${{github.run_id}})
          """
          
          with open('pr-comment.md', 'w') as f:
              f.write(pr_summary)
          EOF
          
          python3 generate-markdown-report.py
          
      - name: Upload unified security report
        uses: actions/upload-artifact@v4
        with:
          name: unified-security-report
          path: |
            unified-security-report.json
            security-report.md
            pr-comment.md
          retention-days: 90
          
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('pr-comment.md', 'utf8');
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Security Scan Results')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }
          
      - name: Security scan summary
        run: |
          echo "================================================"
          echo "Flutter Security Pipeline Execution Complete"
          echo "================================================"
          echo ""
          cat security-report.md
          echo ""
          echo "================================================"
          echo "Artifacts Generated:"
          echo "- Software Bill of Materials (SBOM)"
          echo "- Vulnerability Scan Results"
          echo "- SAST Analysis Reports"
          echo "- Supply Chain Security Report"
          echo "- Unified Security Report"
          echo ""
          echo "View detailed results in GitHub Security tab and workflow artifacts"
          echo "================================================"

      # Validate workflow_dispatch inputs
      - name: Validate workflow_dispatch inputs
        if: github.event_name == 'workflow_dispatch'
        run: |
          if [[ ! "${{ github.event.inputs.scan_type }}" =~ ^(incremental|full|critical-only)$ ]]; then
            echo "Invalid scan_type: ${{ github.event.inputs.scan_type }}"
            exit 1
          fi